{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a2258f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The analytic center is at (0.303311, 0.303311)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define g1\n",
    "def g1(x1, x2):\n",
    "    return 2*x2 - x1\n",
    "\n",
    "# Define g2\n",
    "def g2(x1, x2):\n",
    "    return 2*x1 - x2\n",
    "\n",
    "# Define g3\n",
    "def g3(x1, x2):\n",
    "    return 1 - x1 - x2\n",
    "\n",
    "# Define the Potential Function\n",
    "def potential(x1,x2):\n",
    "    g1_val = g1(x1, x2)\n",
    "    g2_val = g2(x1, x2)\n",
    "    g3_val = g3(x1, x2)\n",
    "    gradient = 1/(g1_val*g2_val*g3_val) # As suggested in the Hint\n",
    "    return gradient\n",
    "\n",
    "#Calculating the gradient of the function by taking the partial derivative wrt to x1 and x2 from the functions g1,g2,g3\n",
    "def gradient_Ψ(x1, x2):\n",
    "    g1_x1 = -2\n",
    "    g1_x2 = 2\n",
    "    g2_x1 = 2\n",
    "    g2_x2 = -2\n",
    "    g3_x1 = -1\n",
    "    g3_x2 = -1\n",
    "    # The denominator is defined so because the value of log(g1(x1,x2))**-1 can be written as 1/g1(x1,x2) as mentioned in the Hint we can minimize f(x) instead of log(f(x))\n",
    "    denominator = (1/g1(x1, x2)*g2(x1, x2)*g3(x1, x2))\n",
    "    # Finding the gradients of x1 and x2 with the help of g1,g2,g3\n",
    "    gradient_x1 = (g1_x1 / denominator + g2_x1 / denominator + g3_x1 / denominator)\n",
    "    gradient_x2 = (g1_x2 / denominator + g2_x2 / denominator + g3_x2 / denominator)\n",
    "    return gradient_x1, gradient_x2\n",
    "\n",
    "#Calculating the Combination Descent(Minimum) of the Function using the Formula New_Value = Old_Value - (Learning_Rate * Slope)\n",
    "def combination_descent(x1,x2):\n",
    "    for i in range(iterations):\n",
    "        gradientx1,gradientx2 = gradient_Ψ(x1, x2)\n",
    "        x1 = x1 - learning_rate * gradientx1\n",
    "        x2 = x2 - learning_rate * gradientx2\n",
    "        if np.linalg.norm(gradient_Ψ(x1, x2)) <= tolerance:\n",
    "            break\n",
    "    return x1/3, x2/3\n",
    "\n",
    "#Setting the Learning Rate to Lower Value\n",
    "learning_rate = 0.1 \n",
    "\n",
    "#Setting Random Value of the number of Iterations\n",
    "iterations = 1000\n",
    "tolerance = 1e-6\n",
    "\n",
    "#Setting the values of x1 & x2 according to the questions\n",
    "x1 = 0.25\n",
    "x2 = 0.25\n",
    "\n",
    "#Calculating the center of S\n",
    "x1, x2 = combination_descent(x1,x2)\n",
    "# Print the analytic center of S \n",
    "print(f\"The analytic center is at ({x1/3:.6f}, {x2/3:.6f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "849ddd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytic center of S (0.23231955447872588,0.11568794567872383)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
